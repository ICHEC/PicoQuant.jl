\section{Evaluations}\label{evaluations}
Here we go through each of the relevant packages and assess their applicability for use in the QuantEx project.

\subsection{cuTENSOR} \label{cutensor}
cuTENSOR\cite{cuTENSOR} is a high performance CUDA library for tensor primitives, with support for tensors of up to rank 12. In cuTENSOR, a tensor is an n-dimensional array that is described by:

\begin{itemize}
\item Its rank i.e.\ the number of modes
\item The extent of each mode
\item Its strides, which are inferred if unspecified
\item Its datatype
\item A unary operation (e.g.\ \texttt{CUTENSOR\_OP\_IDENTITY}) applied to each element
\end{itemize}

\noindent Some of the library's key features, which will be discussed below, include:

\begin{itemize}
\item Element-wise operations
\item Tensor reductions
\item Direct contraction operations (i.e. transpose-free contraction)
\end{itemize}

\noindent For brevity, full CUDA code samples have been omitted here. Detailed examples of element-wise operations, reductions, and contractions may be found in NVIDIA's CUDA library samples git repo\footnote{\url{https://github.com/NVIDIA/CUDALibrarySamples}}. It is to be noted that cuTENSOR requires a relatively new NVIDIA GPU with compute capability of 7.0.


\subsubsection{Element-wise operations} \label{cutensor::elementwise}
There are three element-wise operations provided by cuTENSOR, which will be summarised here.

\vspace{0.5em}\texttt{cutensorPermutation}: performs out-of-place tensor permutations of the form

\begin{equation*} 
B_{\Pi^B(i_0,i_1,...,i_n)} = \alpha\Psi(A_{\Pi^A(i_0,i_1,...,i_n)}),
\end{equation*}

\noindent where $A$ and $B$ are tensors, $\Pi^A$ and $\Pi^B$ are permutation operations that permute the indices of $A$ and $B$, and $\Psi$ is a unary, element-wise operation, such as \texttt{CUTENSOR\_OP\_IDENTITY} or \texttt{CUTENSOR\_OP\_CONJ}, that is specified in the tensor description of $A$.

\vspace{0.5em}\texttt{cutensorElementwiseBinary}: performs the following element-wise operation for two input tensors:

\begin{equation*}
D_{\Pi^C(i_0,i_1,...,i_n)} = \Phi_{AC}(\alpha\Psi_A(A_{\Pi^A(i_0,i_1,...,i_n)}), \gamma\Psi_C(C_{\Pi^C(i_0,i_1,...,i_n)})),
\end{equation*}

\noindent with the notation as described above and $\Phi_{AC}$ being a binary operation such as \texttt{CUTENSOR\_OP\_ADD} or \texttt{CUTENSOR\_OP\_MAX}.

\vspace{0.5em}\texttt{cutensorElementwiseTrinary}: allows a more complex tensor operation that is applied to three input tensors:

\begin{multline*}
D_{\Pi^C(i_0,i_1,...,i_n)} = \Phi_{ABC}(
  \Phi_{AB}(
    \alpha\Psi_A(A_{\Pi^A(i_0,i_1,...,i_n)}),
    \beta\Psi_B(B_{\Pi^B(i_0,i_1,...,i_n)})
  ),\\
  \gamma\Psi_C(C_{\Pi^C(i_0,i_1,...,i_n)})
).
\end{multline*}

\noindent Some example operations of what can be achieved with \texttt{cutensorElementwi\-seTrinary}:
\begin{itemize}
\item $D_{a,b,c,d} = A_{b,d,a,c}$
\item $D_{a,b,c,d} = 2.2 \times A_{b,d,a,c} + 1.3 \times B_{c,b,d,a}$
\item $D_{a,b,c,d} = 2.2 \times A_{b,d,a,c} + 1.3 \times B_{c,b,d,a} + C_{a,b,c,d}$
\item $D_{a,b,c,d} = \mathrm{min}((2.2 \times A_{b,d,a,c} + 1.3 \times B_{c,b,d,a}), C_{a,b,c,d})$
\end{itemize}

\subsubsection{Reduction operations} \label{cutensor::reduction}
The following is a summary of the reduction operations provided by cuTENSOR:

\vspace{0.5em}\texttt{cutensorReduction}: performs general reductions of the form:

\begin{equation*}
D = \alpha \times \mathrm{opReduce}(\Psi_A(A)) + \beta \Psi_C(C).
\end{equation*}

\noindent $\mathrm{opReduce}$ is a binary operation that is used to reduce the elements of $A$. For example, using \texttt{CUTENSOR\_OP\_ADD} will sum over the elements of $\Psi_A(A)$, while \texttt{CUTENSOR\_OP\_MAX} will give the maximum element of $\Psi_A(A)$.

The reduction operation may require some additional scratchpad memory on the device in order to complete. Therefore, the \texttt{cutensorReduction} function has two additional arguments: one being a pointer to the device scratchpad memory and the other being its size. The scratchpad size need not be calculated manually; one may use the \texttt{cutensorReductionGetWorkspace} helper function, which assesses the reduction to be performed using the tensor descriptions and computes the required size. A device buffer of this size must be allocated manually and the resulting pointer passed to \texttt{cutensorReduction}.

Furthermore, partial reductions may be carried out by only specifying a subset of indices:

\begin{equation*}
D_{i,j} = \alpha \times \mathrm{opReduce}(A_{i,j,k}).
\end{equation*}

\subsubsection{Contraction operations} \label{cutensor::contraction}
Following is a summary of the contraction operations provided by cuTENSOR, including functions required to prepare a contraction.

\vspace{0.5em}\texttt{cutensorInitContractionDescription}: this is used to initialise a contraction description object, of type \texttt{cutensorContractionDescriptor\_t}, \\where the tensor contraction is of the form

\begin{equation*}
D_{\mathrm{modes}_D} = \alpha A_{\mathrm{modes}_A} B_{\mathrm{modes}_B} + \beta C_{\mathrm{modes}_C},
\end{equation*}

\noindent using the descriptions of the input tensors $A$, $B$, and $C$, and the output tensor $D$.

\vspace{0.5em}\texttt{cutensorInitContractionFind}: this function may be used to limit the search space of potential contraction algorithms that \texttt{cutensorInitContr\-actionPlan} is allowed to evaluate. \texttt{CUTENSOR\_ALGO\_DEFAULT} lets the heuristic choose the algorithm while other possible options include:

\begin{itemize}
\item \texttt{CUTENSOR\_ALGO\-\_GETT} -- GEMM-like Tensor-Tensor multiplication\footnote{\url{https://dl.acm.org/doi/10.1145/3157733}}
\item \texttt{CUTENSOR\_ALGO\_TGETT} -- Transpose A or B + GETT
\item \texttt{CUTENSOR\_ALGO\_TTGT} -- Transpose-Transpose-GEMM-Transpose
\end{itemize}


\vspace{0.5em}\texttt{cutensorContractionGetWorkspace}: similarly to the reduction case, this helper function is used to determine the required scratch space for a given tensor contraction.

\vspace{0.5em}\texttt{cutensorInitContractionPlan}: initialise the contraction plan for the described contraction on the currently active device. cuTENSORâ€™s heuristic is used to select a candidate plane for a given tensor contraction. As with other libraries, such as FFTW, this plan may be reused multiple times as long as the described contraction remains the same.


\vspace{0.5em}\texttt{cutensorContraction}: performs the planned contraction as set up using \texttt{cutensorInitContractionDescription}. This has the requirement that the active device at kernel invocation is the same one that was active when the plan was initialised.


\vspace{0.5em}\texttt{cutensorContractionMaxAlgos}: this function is used to compute the maximum number of algorithms available to compute tensor contractions\footnote{This function takes no information of the contraction problem so I suspect that perhaps this function is used to probe the capability of the underlying hardware.}. In the case that an algorithm isn't supported, \texttt{cutensorContraction} will return \texttt{CUTENSOR\_STAT\-US\_NOT\_SUPPORTED}. This may be used as an optimisation step in preparing a number of identical contractions, see Listing~\ref{cutensor::optim_example}.

\begin{lstlisting}[language=C++, label=cutensor::optim_example, basicstyle=\small, caption=Finding the most efficient 
algorithm]

int32_t maxAlgosTC = 0;
cutensorContractionMaxAlgos(&maxAlgosTC);

float min_time = std::numeric_limits<float>::max();
int first_algo = static_cast<int>(CUTENSOR_ALGO_GETT);
for (int algo = first_algo; algo < maxAlgosTC; algo++) {
  // Plan and execute contraction using algo measuring
  // the time taken
  
  if (time_taken < min_time) {
    min_time = time_taken;
    optim_algo = algo;
  }
}

// Continue using optim_algo
\end{lstlisting}

\subsubsection{cuTENSOR Contraction Workflow} \label{cutensor::workflow}
The general workflow for completing a tensor contraction is outlined below. The documentation for each library function mentioned, as well as a tensor contraction example code, can be found at \url{https://docs.nvidia.com/cuda/cutensor/api/cutensor.html}.

\begin{enumerate}
\item Create array of indices for each tensor
\item Create map of indices to extents i.e. the size of each labelled dimension
\item Create array of extents for each tensor
\item Copy each tensor to the device
\item Initialise the cuTENSOR library - \texttt{cutensorInit}
\item Create descriptor for each tensor using its size, extents, and a unary operation - \texttt{cutensorInitTensorDescriptor}
\item Compute the necessary alignment for each tensor - \texttt{cuTensorGetAlign\-mentRequirement}
\item Create descriptor for the contraction using the tensor descriptors - \texttt{cutensorInitContractionDescriptor}
\item Find contraction plan - \texttt{cutensorInitContractionFind}
\item Compute the necessary workspace - \texttt{cutensorContractionGetWorksp\-ace}
\item Initialise contraction plan - \texttt{cutensorInitContractionPlan}
\item Execute contraction - \texttt{cutensorContraction}
\item Copy results back to the host
\end{enumerate}

\subsubsection{TensorOperations.jl Integration} \label{cutensor::julia}
cuTENSOR has been integrated into the TensorOperations.jl\footnote{\url{https://github.com/Jutho/TensorOperations.jl}} Julia package. This allows the offloading of tensor contractions to the GPU using similar syntax to the standard TensorOperations contractions, with the difference being the use of the \texttt{@cutensor} macro in place of \texttt{@tensor}. The use of this macro will transfer all arrays to the device before contraction. An example is shown in Listing~\ref{cutensor::julia_example}.

\begin{lstlisting}[language=Julia, label=cutensor::julia_example, caption=cuTENSOR contraction in Julia]
using CuArrays
using TensorOperations

alpha = randn(Float32);
A = randn(Float32,5,5,5,5,5,5);
B = randn(Float32,5,5,5);
C = randn(Float32,5,5,5);
D = zeros(Float32,5,5,5);

@cutensor D[a,b,c] = A[a,e,f,c,f,g]*B[g,b,e] +
			alpha*C[c,a,b]
@cutensor E[a,b,c] := A[a,e,f,c,f,g]*B[g,b,e] +
			alpha*C[c,a,b]
\end{lstlisting}

\noindent In Listing~\ref{cutensor::julia_example}, since the array \texttt{D} exists on the host, it will be copied back at the end of the operation. As \texttt{E} does not, it will remain on the device for further operation. It is to be noted that mixed operations between host and device arrays will fail. Also, as shown in Listing~\ref{cutensor::julia_example}, it is necessary to import CuArrays as failing to do so will raise an error that \texttt{@cutensor} is undefined.

\subsubsection{Future Research} \label{cutensor::futurework}
The following will need to be looked into going forward:

\begin{enumerate}
\item Benchmark cuTENSOR's integration in TensorOperations.jl
\item Operations that don't fit in device memory and investigate further limitations of cuTENSOR
\item Multi-GPU operations
\end{enumerate}

\subsection{qflex}
The qflex simulator received a lot of attention for its use in the google quantum supremacy experiments \cite{Villalonga2019} and managed to reach sustained performance of 281 Pfops/s on the Summit supercomputer at Oakridge National Laboratory.... 

